{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloaders\n",
    "\n",
    "# Create dataloaders for all datasets\n",
    "dataloaders = create_dataloaders(\n",
    "    batch_size=32,\n",
    "    train_split=0.9,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    seed=42,\n",
    "    max_samples=1000\n",
    ")\n",
    "\n",
    "# Print some statistics\n",
    "for dataset_name, loaders in dataloaders.items():\n",
    "    # Calculate total samples for train and test\n",
    "    train_samples = len(loaders['train'].dataset)\n",
    "    test_samples = len(loaders['test'].dataset)\n",
    "    total_samples = train_samples + test_samples\n",
    "    \n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Training samples: {train_samples}\")\n",
    "    print(f\"Test samples: {test_samples}\")\n",
    "    \n",
    "    # Print example sample\n",
    "    batch = next(iter(loaders['train']))\n",
    "    print(\"\\nExample sample:\")\n",
    "    print(f\"Question: {batch['question'][0]}\")\n",
    "    print(f\"Sycophantic answer: {batch['sycophantic_answer'][0]}\")\n",
    "    print(f\"Non-sycophantic answer: {batch['non_sycophantic_answer'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps') #add cpu if needed\n",
    "# Load the Gemma model\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2-2b-it\",\n",
    "    device=device, \n",
    "    dtype=torch.float32 #float16 is faster but less accurate, not suitable for this kind of activaiton/feature analysis\n",
    ")\n",
    "\n",
    "print(f\"Number of layers in the model: {model.cfg.n_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activations import collect_and_save_activations, load_activation_dataloaders\n",
    "\n",
    "save_dir = \".data/activations\"\n",
    "hooks=[]\n",
    "for layer in range(0,model.cfg.n_layers):\n",
    "        hooks.extend([\n",
    "            # f'blocks.{layer}.hook_resid_pre',  # Before attention\n",
    "            # f'blocks.{layer}.hook_resid_mid',  # After attention, before MLP\n",
    "            f'blocks.{layer}.hook_resid_post'  # After MLP\n",
    "        ])\n",
    "print(len(hooks))\n",
    "\n",
    "# Collect activations for each dataset\n",
    "\n",
    "collect_and_save_activations(\n",
    "    model=model,\n",
    "    train_dataloader=dataloaders['mixed']['train'],\n",
    "    test_dataloader=dataloaders['mixed']['test'],\n",
    "    hooks=hooks,\n",
    "    save_dir=save_dir,\n",
    "    dataset_name='mixed',\n",
    "    print_outputs=False\n",
    ")\n",
    "collect_and_save_activations(\n",
    "    model=model,\n",
    "    train_dataloader=dataloaders['politics']['train'],\n",
    "    test_dataloader=dataloaders['politics']['test'], \n",
    "    hooks=hooks,\n",
    "    save_dir=save_dir,\n",
    "    dataset_name='politics',\n",
    "    print_outputs=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & test Probes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe import train_probe, evaluate_probe, save_probe_and_results\n",
    "\n",
    "# Load activation dataloaders\n",
    "for hook in hooks:\n",
    "    # train on nlp and philosophy\n",
    "    mixed_activation_loaders = load_activation_dataloaders(\n",
    "        save_dir,\n",
    "        model.cfg.model_name,\n",
    "        \"mixed\", \n",
    "        hook=hook,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Train probe\n",
    "    probe, losses = train_probe(\n",
    "        mixed_activation_loaders['train'],\n",
    "        input_dim=2304 ,  # Model's hidden dimension, 2048 for OG gemma\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "\n",
    "    # Evaluate probe\n",
    "    # test on nlp and philosophy\n",
    "    mixed_results = evaluate_probe(probe, mixed_activation_loaders['test'], device)\n",
    "    #test on politics\n",
    "    politics_activation_loaders = load_activation_dataloaders(  \n",
    "        save_dir,\n",
    "        model.cfg.model_name,\n",
    "        \"politics\", \n",
    "        hook=hook,\n",
    "        batch_size=32\n",
    "    )\n",
    "    politics_results = evaluate_probe(probe, politics_activation_loaders['test'], device)\n",
    "    results = {\n",
    "        'mixed': mixed_results,\n",
    "        'politics': politics_results\n",
    "    }\n",
    "    save_probe_and_results(\n",
    "        save_dir=save_dir,\n",
    "        model_name=model.cfg.model_name,\n",
    "        dataset_name='mixed',  # dataset used for training\n",
    "        hook=hook,\n",
    "        probe=probe,\n",
    "        results=results,\n",
    "        losses=losses\n",
    "    )\n",
    "    \n",
    "\n",
    "   \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Hook: {hook}\")\n",
    "    print(f\"Accuracy: {mixed_results['accuracy']:.2%}\")\n",
    "    print(f\"Total samples: {mixed_results['total_samples']}\")\n",
    "    print(f\"Pol Accuracy: {politics_results['accuracy']:.2%}\")\n",
    "    print(f\"Pol Total samples: {politics_results['total_samples']}\")\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from probe import load_probe_results\n",
    "\n",
    "def plot_accuracies_by_layer(save_dir: str, model_name: str, dataset_name: str = 'mixed'):\n",
    "    \"\"\"\n",
    "    Plot test accuracies across layers for both mixed and politics datasets.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory where results are saved\n",
    "        model_name: Name of the model\n",
    "        dataset_name: Name of the dataset used for training probes (default: 'mixed')\n",
    "    \"\"\"\n",
    "    # Load results\n",
    "    results = load_probe_results(save_dir, model_name, dataset_name)\n",
    "    \n",
    "    # Extract layers and accuracies\n",
    "    layers = sorted(results[dataset_name].keys())\n",
    "    mixed_accs = [results[dataset_name][layer]['results']['mixed']['accuracy'] for layer in layers]\n",
    "    pol_accs = [results[dataset_name][layer]['results']['politics']['accuracy'] for layer in layers]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(layers, mixed_accs, 'b-o', label='Mixed Dataset')\n",
    "    plt.plot(layers, pol_accs, 'r-o', label='Politics Dataset')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Probe Accuracy by Layer ({model_name})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add horizontal line at 0.5 for random chance\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Set y-axis limits with some padding\n",
    "    plt.ylim(0.3, 1.1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Mixed Dataset - Mean Accuracy: {np.mean(mixed_accs):.3f}\")\n",
    "    print(f\"Politics Dataset - Mean Accuracy: {np.mean(pol_accs):.3f}\")\n",
    "    print(f\"\\nBest Layer for Mixed: {layers[np.argmax(mixed_accs)]} (Acc: {max(mixed_accs):.3f})\")\n",
    "    print(f\"Best Layer for Politics: {layers[np.argmax(pol_accs)]} (Acc: {max(pol_accs):.3f})\")\n",
    "plot_accuracies_by_layer(save_dir, model.cfg.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation measuring comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
