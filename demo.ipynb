{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloaders\n",
    "\n",
    "# Create dataloaders for all datasets\n",
    "dataloaders = create_dataloaders(\n",
    "    batch_size=32,\n",
    "    train_split=0.9,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    seed=42,\n",
    "    max_samples=100\n",
    ")\n",
    "\n",
    "# Print some statistics\n",
    "for dataset_name, loaders in dataloaders.items():\n",
    "    # Calculate total samples for train and test\n",
    "    train_samples = len(loaders['train'].dataset)\n",
    "    test_samples = len(loaders['test'].dataset)\n",
    "    total_samples = train_samples + test_samples\n",
    "    \n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Training samples: {train_samples}\")\n",
    "    print(f\"Test samples: {test_samples}\")\n",
    "    \n",
    "    # Print example sample\n",
    "    batch = next(iter(loaders['train']))\n",
    "    print(\"\\nExample sample:\")\n",
    "    print(f\"Question: {batch['question'][0]}\")\n",
    "    print(f\"Sycophantic answer: {batch['sycophantic_answer'][0]}\")\n",
    "    print(f\"Non-sycophantic answer: {batch['non_sycophantic_answer'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps') #add cpu if needed\n",
    "# Load the Gemma model\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2b-it\",\n",
    "    device=device, \n",
    "    dtype=torch.float16 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "from dataset import create_dataloaders\n",
    "from activations import collect_and_save_activations, load_activation_dataloaders\n",
    "from probe import train_probe, evaluate_probe\n",
    "\n",
    "save_dir = \".data/activations\"\n",
    "hooks=[]\n",
    "for layer in range(model.cfg.n_layers):\n",
    "        hooks.extend([\n",
    "            f'blocks.{layer}.hook_resid_pre',  # Before attention\n",
    "            f'blocks.{layer}.hook_resid_mid',  # After attention, before MLP\n",
    "            f'blocks.{layer}.hook_resid_post'  # After MLP\n",
    "        ])\n",
    "print(len(hooks))\n",
    "# Collect activations for each dataset\n",
    "for dataset_name, loaders in dataloaders.items():\n",
    "    collect_and_save_activations(\n",
    "            model,\n",
    "            loaders['train'],\n",
    "            hooks,\n",
    "            save_dir,\n",
    "            dataset_name\n",
    "        )\n",
    "\n",
    "# Load activation dataloaders\n",
    "activation_loaders = load_activation_dataloaders(\n",
    "    save_dir,\n",
    "    model.cfg.model_name,\n",
    "    \"nlp\", \n",
    "    hook=hooks[0],\n",
    "    batch_size=32\n",
    ")\n",
    "# Free up GPU memory\n",
    "# del model\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "# Train probe\n",
    "probe, losses = train_probe(\n",
    "    activation_loaders['train'],\n",
    "    input_dim=2048,  # Model's hidden dimension\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# # Evaluate probe\n",
    "# results = evaluate_probe(probe, activation_loaders['test'], device)\n",
    "# print(f\"\\nTest Results:\")\n",
    "# print(f\"Accuracy: {results['accuracy']:.2%}\")\n",
    "# print(f\"Total samples: {results['total_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_loaders = load_activation_dataloaders(\n",
    "    save_dir,\n",
    "    \"nlp\",  # Or whichever dataset you want to use\n",
    "    layer,\n",
    "    hook=hooks[0],\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test probe on multiple datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation measuring comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
