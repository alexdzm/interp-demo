{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloaders\n",
    "\n",
    "# Create dataloaders for all datasets\n",
    "dataloaders = create_dataloaders(\n",
    "    batch_size=32,\n",
    "    train_split=0.9,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    seed=42,\n",
    "    max_samples=100\n",
    ")\n",
    "\n",
    "# Print some statistics\n",
    "for dataset_name, loaders in dataloaders.items():\n",
    "    # Calculate total samples for train and test\n",
    "    train_samples = len(loaders['train'].dataset)\n",
    "    test_samples = len(loaders['test'].dataset)\n",
    "    total_samples = train_samples + test_samples\n",
    "    \n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Training samples: {train_samples}\")\n",
    "    print(f\"Test samples: {test_samples}\")\n",
    "    \n",
    "    # Print example sample\n",
    "    batch = next(iter(loaders['train']))\n",
    "    print(\"\\nExample sample:\")\n",
    "    print(f\"Question: {batch['question'][0]}\")\n",
    "    print(f\"Sycophantic answer: {batch['sycophantic_answer'][0]}\")\n",
    "    print(f\"Non-sycophantic answer: {batch['non_sycophantic_answer'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Load the Gemma model\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2b-it\",\n",
    "    device='cuda' if torch.cuda.is_available() else 'mps',\n",
    "    dtype=torch.float16  # Use float16 to save memory\n",
    ")\n",
    "\n",
    "def get_resid_stream_activations(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str\n",
    ") -> Tuple[torch.Tensor, ActivationCache]:\n",
    "    \"\"\"\n",
    "    Get activations from the residual stream at each layer.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (logits, cache) where cache contains activations\n",
    "    \"\"\"\n",
    "    # Define activation names we want to cache\n",
    "    activation_names = []\n",
    "    \n",
    "    # Get residual stream before and after each attention and MLP block\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        activation_names.extend([\n",
    "            f'blocks.{layer}.hook_resid_pre',  # Before attention\n",
    "            f'blocks.{layer}.hook_resid_mid',  # After attention, before MLP\n",
    "            f'blocks.{layer}.hook_resid_post'  # After MLP\n",
    "        ])\n",
    "    \n",
    "    # Run model with caching\n",
    "    logits, cache = model.run_with_cache(\n",
    "        prompt,\n",
    "        names_filter=activation_names,\n",
    "    )\n",
    "    \n",
    "    return logits, cache\n",
    "\n",
    "\n",
    "prompt = \"What is machine learning?\"\n",
    "logits, cache = get_resid_stream_activations(model, prompt)\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Number of layers: {model.cfg.n_layers}\")\n",
    "print(f\"Available activation keys:\", cache.cache_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "layer_to_probe = 10  # Choose which layer to probe\n",
    "\n",
    "# Train probe\n",
    "train_loader = dataloaders['nlp']['train']\n",
    "probe, training_losses = train_probe(model, train_loader, layer_to_probe, device)\n",
    "\n",
    "# Evaluate probe\n",
    "test_loader = dataloaders['nlp']['test']\n",
    "results = evaluate_probe(model, probe, test_loader, layer_to_probe, device)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.2%}\")\n",
    "print(f\"Total samples: {results['total_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test probe on multiple datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation measuring comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
