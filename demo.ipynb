{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()    \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloaders\n",
    "\n",
    "# Create dataloaders for all datasets\n",
    "dataloaders = create_dataloaders(\n",
    "    batch_size=32,\n",
    "    train_split=0.9,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    seed=42,\n",
    "    max_samples=1000\n",
    ")\n",
    "\n",
    "# Print some statistics\n",
    "for dataset_name, loaders in dataloaders.items():\n",
    "    # Calculate total samples for train and test\n",
    "    train_samples = len(loaders['train'].dataset)\n",
    "    test_samples = len(loaders['test'].dataset)\n",
    "    total_samples = train_samples + test_samples\n",
    "    \n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Training samples: {train_samples}\")\n",
    "    print(f\"Test samples: {test_samples}\")\n",
    "    \n",
    "    # Print example sample\n",
    "    batch = next(iter(loaders['train']))\n",
    "    print(\"\\nExample sample:\")\n",
    "    print(f\"Question: {batch['question'][0]}\")\n",
    "    print(f\"Sycophantic answer: {batch['sycophantic_answer'][0]}\")\n",
    "    print(f\"Non-sycophantic answer: {batch['non_sycophantic_answer'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Whitebox Baseline\n",
    "\n",
    "Needs an API key for openai aet aas an env variable to run. dotenv runs automatically so you can create an env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from judge_eval import SycophancyJudge\n",
    "import os\n",
    "save_dir = \".data/judge_eval\"\n",
    "judge = SycophancyJudge(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "results = {}\n",
    "for dataset_name, loaders in dataloaders.items():\n",
    "    print(f\"\\nEvaluating {dataset_name} dataset...\")\n",
    "    result = judge.evaluate_dataset(loaders['test'])\n",
    "    results[dataset_name] = result\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Accuracy: {result.accuracy:.2%} (95% CI: {result.ci_lower:.2%}-{result.ci_upper:.2%})\")\n",
    "    print(f\"True Positives: {result.true_positives}\")\n",
    "    print(f\"False Positives: {result.false_positives}\")\n",
    "    print(f\"True Negatives: {result.true_negatives}\")\n",
    "    print(f\"False Negatives: {result.false_negatives}\")\n",
    "\n",
    "# Save results\n",
    "judge.save_results(save_dir, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps') #add cpu if needed\n",
    "# Load the Gemma model\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2-2b\",\n",
    "    device=device, \n",
    "    dtype=torch.float32 #float16 is faster but less accurate, not suitable for this kind of activaiton/feature analysis\n",
    ")\n",
    "\n",
    "print(f\"Number of layers in the model: {model.cfg.n_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activations import collect_and_save_activations, load_activation_dataloaders\n",
    "\n",
    "save_dir = \".data/activations\"\n",
    "hooks=[]\n",
    "for layer in range(0,model.cfg.n_layers):\n",
    "        hooks.extend([\n",
    "            # f'blocks.{layer}.hook_resid_pre',  # Before attention\n",
    "            # f'blocks.{layer}.hook_resid_mid',  # After attention, before MLP\n",
    "            f'blocks.{layer}.hook_resid_post'  # After MLP\n",
    "        ])\n",
    "print(len(hooks))\n",
    "\n",
    "# Collect activations for each dataset\n",
    "\n",
    "collect_and_save_activations(\n",
    "    model=model,\n",
    "    train_dataloader=dataloaders['mixed']['train'],\n",
    "    test_dataloader=dataloaders['mixed']['test'],\n",
    "    hooks=hooks,\n",
    "    save_dir=save_dir,\n",
    "    dataset_name='mixed',\n",
    "    print_outputs=False\n",
    ")\n",
    "collect_and_save_activations(\n",
    "    model=model,\n",
    "    train_dataloader=dataloaders['politics']['train'],\n",
    "    test_dataloader=dataloaders['politics']['test'], \n",
    "    hooks=hooks,\n",
    "    save_dir=save_dir,\n",
    "    dataset_name='politics',\n",
    "    print_outputs=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & test Probes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe import train_probe, evaluate_probe, save_probe_and_results\n",
    "\n",
    "# Load activation dataloaders\n",
    "for hook in hooks:\n",
    "    # train on nlp and philosophy\n",
    "    mixed_activation_loaders = load_activation_dataloaders(\n",
    "        save_dir,\n",
    "        model.cfg.model_name,\n",
    "        \"mixed\", \n",
    "        hook=hook,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Train probe\n",
    "    probe, losses = train_probe(\n",
    "        mixed_activation_loaders['train'],\n",
    "        input_dim=2304 ,  # Model's hidden dimension, 2048 for OG gemma\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "\n",
    "    # Evaluate probe\n",
    "    # test on nlp and philosophy\n",
    "    mixed_results = evaluate_probe(probe, mixed_activation_loaders['test'], device)\n",
    "    #test on politics\n",
    "    politics_activation_loaders = load_activation_dataloaders(  \n",
    "        save_dir,\n",
    "        model.cfg.model_name,\n",
    "        \"politics\", \n",
    "        hook=hook,\n",
    "        batch_size=32\n",
    "    )\n",
    "    politics_results = evaluate_probe(probe, politics_activation_loaders['test'], device)\n",
    "    results = {\n",
    "        'mixed': mixed_results,\n",
    "        'politics': politics_results\n",
    "    }\n",
    "    save_probe_and_results(\n",
    "        save_dir=save_dir,\n",
    "        model_name=model.cfg.model_name,\n",
    "        dataset_name='mixed',  # dataset used for training\n",
    "        hook=hook,\n",
    "        probe=probe,\n",
    "        results=results,\n",
    "        losses=losses\n",
    "    )\n",
    "    \n",
    "\n",
    "   \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Hook: {hook}\")\n",
    "    print(f\"Accuracy: {mixed_results['accuracy']:.2%}\")\n",
    "    print(f\"Total samples: {mixed_results['total_samples']}\")\n",
    "    print(f\"Pol Accuracy: {politics_results['accuracy']:.2%}\")\n",
    "    print(f\"Pol Total samples: {politics_results['total_samples']}\")\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe import plot_accuracies_by_layer\n",
    "\n",
    "\n",
    "plot_accuracies_by_layer(save_dir, model.cfg.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_analysis import main\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
